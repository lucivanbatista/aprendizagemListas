{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn import linear_model, metrics, datasets\n",
    "from sklearn.svm import LinearSVR\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from mylibs import transform as tf\n",
    "from mylibs import resample as rs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 01. Crie os seguintes arquivos com extensão .py e implemente os métodos definidos para cada um deles:\n",
    "* transform.py\n",
    "    - standardize\n",
    "    - normalize\n",
    "* resample.py\n",
    "    - split_k_fold(n_elem, n_splits=3, shuffle=True, seed=0)\n",
    "    - n_elem - número total de elementos.\n",
    "    - n_split - número de folds. Mínimo: 2.\n",
    "    - shuffle - aleatoriza a ordem dos dados (True) ou não (False).\n",
    "    - seed - determina uma semente para geração de números aleatórios ou não (None).\n",
    "    - Retorno: 2 arrays (idx_train e idx_test), cada um com n_splits elementos: \n",
    "    - um com os índices de treino. Exemplo para n_splits=3, teremos idx_train[0], idx_train[1] e idx_train[2].\n",
    "    - um com os índices de teste. Exemplo para n_splits=3, teremos idx_test[0], idx_test[1] e idx_test[2]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "x = np.random.rand(20) # 20 valores\n",
    "x = (x * 100).round(2) # valores até 100\n",
    "x = np.resize(x, (20, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.56025437],\n",
       "       [0.73661897],\n",
       "       [0.61748808],\n",
       "       [0.55612083],\n",
       "       [0.42766296],\n",
       "       [0.66316905],\n",
       "       [0.44239534],\n",
       "       [0.92379438],\n",
       "       [1.        ],\n",
       "       [0.38494966],\n",
       "       [0.81770005],\n",
       "       [0.53916269],\n",
       "       [0.58060413],\n",
       "       [0.95961844],\n",
       "       [0.05384208],\n",
       "       [0.0709062 ],\n",
       "       [0.        ],\n",
       "       [0.86104928],\n",
       "       [0.80339163],\n",
       "       [0.90068892]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.normalize(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.11870903],\n",
       "       [ 0.48434953],\n",
       "       [ 0.07699507],\n",
       "       [-0.13284322],\n",
       "       [-0.5720902 ],\n",
       "       [ 0.23319593],\n",
       "       [-0.52171451],\n",
       "       [ 1.12437442],\n",
       "       [ 1.38495081],\n",
       "       [-0.71814345],\n",
       "       [ 0.761597  ],\n",
       "       [-0.19082962],\n",
       "       [-0.04912535],\n",
       "       [ 1.24687069],\n",
       "       [-1.85032791],\n",
       "       [-1.79197909],\n",
       "       [-2.03443473],\n",
       "       [ 0.90982474],\n",
       "       [ 0.71267098],\n",
       "       [ 1.04536795]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.standardize(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test, train = rs.slipt_k_fold(20, 5, True, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ 2,  8,  6, 19, 15,  4, 16, 12,  9,  1,  0,  7, 14, 17,  3, 18]),\n",
       " array([13, 11, 10,  5, 15,  4, 16, 12,  9,  1,  0,  7, 14, 17,  3, 18]),\n",
       " array([13, 11, 10,  5,  2,  8,  6, 19,  9,  1,  0,  7, 14, 17,  3, 18]),\n",
       " array([13, 11, 10,  5,  2,  8,  6, 19, 15,  4, 16, 12, 14, 17,  3, 18]),\n",
       " array([13, 11, 10,  5,  2,  8,  6, 19, 15,  4, 16, 12,  9,  1,  0,  7])]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([13, 11, 10,  5]),\n",
       " array([ 2,  8,  6, 19]),\n",
       " array([15,  4, 16, 12]),\n",
       " array([9, 1, 0, 7]),\n",
       " array([14, 17,  3, 18])]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carregando Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1599, 12)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('winequality-red.csv', delimiter=';')\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>407</th>\n",
       "      <td>12.0</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.66</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.093</td>\n",
       "      <td>12.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.99960</td>\n",
       "      <td>3.18</td>\n",
       "      <td>0.63</td>\n",
       "      <td>10.8</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1220</th>\n",
       "      <td>10.9</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.52</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0.132</td>\n",
       "      <td>17.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>0.99734</td>\n",
       "      <td>3.28</td>\n",
       "      <td>0.77</td>\n",
       "      <td>11.5</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1200</th>\n",
       "      <td>7.7</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.21</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.069</td>\n",
       "      <td>4.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.99458</td>\n",
       "      <td>3.16</td>\n",
       "      <td>0.54</td>\n",
       "      <td>9.8</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>308</th>\n",
       "      <td>10.3</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.44</td>\n",
       "      <td>2.4</td>\n",
       "      <td>0.214</td>\n",
       "      <td>5.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.99940</td>\n",
       "      <td>3.19</td>\n",
       "      <td>0.63</td>\n",
       "      <td>9.5</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1328</th>\n",
       "      <td>6.5</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.11</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0.073</td>\n",
       "      <td>13.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>0.99550</td>\n",
       "      <td>3.34</td>\n",
       "      <td>0.52</td>\n",
       "      <td>9.3</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "407            12.0              0.39         0.66             3.0      0.093   \n",
       "1220           10.9              0.32         0.52             1.8      0.132   \n",
       "1200            7.7              0.57         0.21             1.5      0.069   \n",
       "308            10.3              0.43         0.44             2.4      0.214   \n",
       "1328            6.5              0.52         0.11             1.8      0.073   \n",
       "\n",
       "      free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
       "407                  12.0                  30.0  0.99960  3.18       0.63   \n",
       "1220                 17.0                  44.0  0.99734  3.28       0.77   \n",
       "1200                  4.0                   9.0  0.99458  3.16       0.54   \n",
       "308                   5.0                  12.0  0.99940  3.19       0.63   \n",
       "1328                 13.0                  38.0  0.99550  3.34       0.52   \n",
       "\n",
       "      alcohol  quality  \n",
       "407      10.8        7  \n",
       "1220     11.5        6  \n",
       "1200      9.8        6  \n",
       "308       9.5        6  \n",
       "1328      9.3        5  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1599.000000</td>\n",
       "      <td>1599.000000</td>\n",
       "      <td>1599.000000</td>\n",
       "      <td>1599.000000</td>\n",
       "      <td>1599.000000</td>\n",
       "      <td>1599.000000</td>\n",
       "      <td>1599.000000</td>\n",
       "      <td>1599.000000</td>\n",
       "      <td>1599.000000</td>\n",
       "      <td>1599.000000</td>\n",
       "      <td>1599.000000</td>\n",
       "      <td>1599.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>8.319637</td>\n",
       "      <td>0.527821</td>\n",
       "      <td>0.270976</td>\n",
       "      <td>2.538806</td>\n",
       "      <td>0.087467</td>\n",
       "      <td>15.874922</td>\n",
       "      <td>46.467792</td>\n",
       "      <td>0.996747</td>\n",
       "      <td>3.311113</td>\n",
       "      <td>0.658149</td>\n",
       "      <td>10.422983</td>\n",
       "      <td>5.636023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.741096</td>\n",
       "      <td>0.179060</td>\n",
       "      <td>0.194801</td>\n",
       "      <td>1.409928</td>\n",
       "      <td>0.047065</td>\n",
       "      <td>10.460157</td>\n",
       "      <td>32.895324</td>\n",
       "      <td>0.001887</td>\n",
       "      <td>0.154386</td>\n",
       "      <td>0.169507</td>\n",
       "      <td>1.065668</td>\n",
       "      <td>0.807569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>4.600000</td>\n",
       "      <td>0.120000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.012000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.990070</td>\n",
       "      <td>2.740000</td>\n",
       "      <td>0.330000</td>\n",
       "      <td>8.400000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>7.100000</td>\n",
       "      <td>0.390000</td>\n",
       "      <td>0.090000</td>\n",
       "      <td>1.900000</td>\n",
       "      <td>0.070000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>0.995600</td>\n",
       "      <td>3.210000</td>\n",
       "      <td>0.550000</td>\n",
       "      <td>9.500000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>7.900000</td>\n",
       "      <td>0.520000</td>\n",
       "      <td>0.260000</td>\n",
       "      <td>2.200000</td>\n",
       "      <td>0.079000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>0.996750</td>\n",
       "      <td>3.310000</td>\n",
       "      <td>0.620000</td>\n",
       "      <td>10.200000</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>9.200000</td>\n",
       "      <td>0.640000</td>\n",
       "      <td>0.420000</td>\n",
       "      <td>2.600000</td>\n",
       "      <td>0.090000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>0.997835</td>\n",
       "      <td>3.400000</td>\n",
       "      <td>0.730000</td>\n",
       "      <td>11.100000</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>15.900000</td>\n",
       "      <td>1.580000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>15.500000</td>\n",
       "      <td>0.611000</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>289.000000</td>\n",
       "      <td>1.003690</td>\n",
       "      <td>4.010000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>14.900000</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       fixed acidity  volatile acidity  citric acid  residual sugar  \\\n",
       "count    1599.000000       1599.000000  1599.000000     1599.000000   \n",
       "mean        8.319637          0.527821     0.270976        2.538806   \n",
       "std         1.741096          0.179060     0.194801        1.409928   \n",
       "min         4.600000          0.120000     0.000000        0.900000   \n",
       "25%         7.100000          0.390000     0.090000        1.900000   \n",
       "50%         7.900000          0.520000     0.260000        2.200000   \n",
       "75%         9.200000          0.640000     0.420000        2.600000   \n",
       "max        15.900000          1.580000     1.000000       15.500000   \n",
       "\n",
       "         chlorides  free sulfur dioxide  total sulfur dioxide      density  \\\n",
       "count  1599.000000          1599.000000           1599.000000  1599.000000   \n",
       "mean      0.087467            15.874922             46.467792     0.996747   \n",
       "std       0.047065            10.460157             32.895324     0.001887   \n",
       "min       0.012000             1.000000              6.000000     0.990070   \n",
       "25%       0.070000             7.000000             22.000000     0.995600   \n",
       "50%       0.079000            14.000000             38.000000     0.996750   \n",
       "75%       0.090000            21.000000             62.000000     0.997835   \n",
       "max       0.611000            72.000000            289.000000     1.003690   \n",
       "\n",
       "                pH    sulphates      alcohol      quality  \n",
       "count  1599.000000  1599.000000  1599.000000  1599.000000  \n",
       "mean      3.311113     0.658149    10.422983     5.636023  \n",
       "std       0.154386     0.169507     1.065668     0.807569  \n",
       "min       2.740000     0.330000     8.400000     3.000000  \n",
       "25%       3.210000     0.550000     9.500000     5.000000  \n",
       "50%       3.310000     0.620000    10.200000     6.000000  \n",
       "75%       3.400000     0.730000    11.100000     6.000000  \n",
       "max       4.010000     2.000000    14.900000     8.000000  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 5\n",
    "idx_test, idx_train = rs.slipt_k_fold(data.shape[0], k, True, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([1455,  788,  355,  338,  433,   68,  820, 1198,  107, 1340, 1194,\n",
       "         981,  101, 1034,  549, 1197,  739, 1172,  555, 1289,  735,  675,\n",
       "        1486, 1107,  451, 1278, 1432, 1208,  240,  373,  643, 1053, 1512,\n",
       "         691,  402,  463, 1196, 1226,  960,  218,  529,  187,  508, 1293,\n",
       "         312,  322,  877,  143,  450,   15,  626,  939,  560, 1073, 1490,\n",
       "         580,  905,  772,  332, 1087, 1480,  780,  315,  607, 1371,  747,\n",
       "        1582,  147,  177,   32,  115,  192, 1125, 1064,  188,  158, 1082,\n",
       "         969,  121,  904, 1532,  155,  599, 1162, 1538,  480,  609,  587,\n",
       "         982,  499,  845,  424, 1373,  807, 1530, 1424,   75, 1300,  149,\n",
       "         230,  171, 1231,  934, 1007,   27,  900,  259, 1276,  511, 1155,\n",
       "         682,  742,  929, 1355, 1377, 1063, 1047,  584,  764,  270, 1014,\n",
       "         746,  198,  832, 1001,  319, 1126,  874,   91,  427,  776,   16,\n",
       "        1405, 1446,  543,  848, 1408, 1586,  627, 1274, 1042, 1429, 1351,\n",
       "         784, 1334,  754,  290,  592, 1458,  894,  105,  556,  461, 1216,\n",
       "         886, 1119,  152,  537,  686,  164,  571, 1070,  974,  428, 1303,\n",
       "         502,  175, 1443,  456,  265,  261, 1232, 1555,   21,  110, 1510,\n",
       "         909, 1247, 1138,  368, 1379, 1541, 1460,  236,  683,  160,  808,\n",
       "         636, 1031,   79, 1423,  654, 1444,  880, 1086,  923,  548,  544,\n",
       "         586,  657,  716, 1117,  122, 1464,  797, 1106,   77, 1483,  811,\n",
       "          46,  672,  551,  896, 1487, 1287,  734,   18, 1323,  416,  861,\n",
       "        1547, 1175,   54,  142,  928, 1187,  190,  141,  441,  377,  229,\n",
       "        1286, 1573,  554,   62,  639,  670,  313,  574,  755,  774,   63,\n",
       "        1584,  347,  191, 1025,  699,  366, 1551,    9,  604,  582, 1298,\n",
       "         567, 1161,  867, 1484,  623,  196,   98,  736,  282, 1404,  245,\n",
       "        1206,  341,  671, 1347,  234, 1310,  336, 1365,  702,  762,  434,\n",
       "         782, 1052, 1343,  449,  849, 1103, 1251, 1191,  619,  714, 1577,\n",
       "        1558, 1400,  895,  758, 1478,  498, 1537,  611,  810, 1496,  561,\n",
       "        1049, 1178,  527, 1314,  618, 1507, 1217, 1000,  206,  957,  207,\n",
       "         280,   94,  125, 1321, 1248,  112,  157,   57,  679, 1319,  505,\n",
       "         476]),\n",
       " array([1084,  741,  520, 1282, 1091,  863, 1445, 1145,  334, 1463, 1115,\n",
       "         304,  241, 1588,  553, 1398, 1534,   81,  603, 1171, 1253,  120,\n",
       "         966,  472, 1118,  507, 1094, 1174,  384,  631,  936,  500,  453,\n",
       "         938, 1190,  879,  224,  778, 1596,  868, 1317, 1035, 1130,  876,\n",
       "         204, 1219,  677, 1189, 1002, 1133,  318,  468, 1517,  131, 1136,\n",
       "        1312, 1302, 1559,   84, 1122, 1527, 1539,  132, 1292,  655,  829,\n",
       "         506,   76, 1524,  380,   88,  359, 1269,  488,  799, 1207,   93,\n",
       "        1370, 1359,  137,  947,  521,  258, 1135,  298, 1412,  950,  415,\n",
       "        1525,   95,  260,  674,   96, 1322, 1211,  878,  830,    1,   20,\n",
       "        1256,  251, 1112,   64, 1090,  186,  930, 1041,  964,   72,  227,\n",
       "        1567, 1593, 1188, 1285,  823,  668, 1494, 1434,  989,  139,  371,\n",
       "         446,  129,  475,  986,  624,  557, 1491,  494, 1170, 1339, 1296,\n",
       "        1394,  221,  317,  535, 1239,  523,  545,  866,  151,  133,  533,\n",
       "        1461,  786,  847,  856, 1176,  510,  144,    4, 1332,  857, 1311,\n",
       "          35,   69, 1306, 1271, 1482,  723,  103,  959,  595,  185,  610,\n",
       "        1114, 1428, 1587,  640,  692,  219, 1422,  438,  514, 1121,  818,\n",
       "          48, 1383, 1169,  839,  253, 1462,  357,   56,  331,  233,   82,\n",
       "        1479,  883,   73,   60,   23,  406, 1065,  901,  649,  531,  920,\n",
       "          49,  732,  327, 1223,  109,  184, 1111, 1308,  822, 1367,  773,\n",
       "        1288, 1553, 1407,  536, 1440,  576, 1029,   90,  361,  946, 1313,\n",
       "         134,  403, 1326,  388, 1349,   31, 1513, 1280, 1077,  585,  102,\n",
       "         707,  390, 1183, 1318,  130, 1069, 1127, 1562,  724,  467, 1416,\n",
       "        1591,  401,  252,  659,  263,  225,  140, 1083, 1004,  273,  978,\n",
       "         442,   42, 1157,  605, 1050,  393, 1262,  887, 1516,  431,  869,\n",
       "        1098, 1044,  353, 1346, 1403,  515, 1439, 1426, 1362,  902, 1267,\n",
       "        1336,  350, 1375,  661, 1036,  838,  276,  972,  859, 1378,  417,\n",
       "         328,  111, 1164, 1104,  798,  148, 1071,  375,  530,  662,  831,\n",
       "         663,  628,  268,  100,  358,  819, 1236, 1550, 1561,  363, 1147,\n",
       "        1421,   33,  921, 1492,  899, 1260,  558,  278,  512,  182,  362,\n",
       "        1447]),\n",
       " array([ 681,  932,  767,  836,  685, 1341, 1540, 1565,  948, 1590,  766,\n",
       "         385, 1295, 1209,  193,   92,  469,  396,  178, 1451,  814,  984,\n",
       "        1202,  841,  676,  858, 1521,  526, 1080,  474,  749,  967,  652,\n",
       "         720,  159,  944,  128, 1401,  286,  824,  267, 1092, 1589,  135,\n",
       "         126,  397,  793, 1526,  954,   10, 1249,  579,  815, 1139, 1324,\n",
       "         918, 1358,  846,  209, 1392, 1519, 1470,  733,  761,  706,  907,\n",
       "           0,  563, 1307,  199, 1449,   14,  208,  760, 1018,  800, 1533,\n",
       "         852,  842,  955,  646,  596, 1495,  581,   74,  621, 1116,  119,\n",
       "         189, 1433, 1233,  294, 1056,   29, 1229,  889, 1442, 1177, 1284,\n",
       "           8,  310,   13,  927,  477,  309, 1181, 1168,  684,  504, 1469,\n",
       "         354,  698,  958,  805,   37,   12, 1357, 1515, 1505,  941, 1560,\n",
       "         790, 1265,  176, 1009, 1489, 1395,  787, 1493, 1100, 1245, 1309,\n",
       "        1331,  495,  418,  781,  744,  547, 1294, 1297,  865,  577,  214,\n",
       "          28, 1275,  658, 1290,  314, 1595,  725,  202,  395, 1522,  992,\n",
       "        1344, 1097,  320,  834,  647,  369,  194, 1415,  501,   11,  613,\n",
       "        1500,  729, 1022,  765, 1472, 1038, 1438, 1237,  297, 1430,  665,\n",
       "         882, 1224,  299, 1411,  296, 1575,  232,   71,  287, 1266, 1101,\n",
       "         150,  528, 1242, 1544,  862,  638, 1581, 1201,  642,  237,  937,\n",
       "         562,  906,  891,  285, 1427, 1572,  457, 1153, 1166, 1291,  392,\n",
       "         988,  307, 1120,  943,  113,  360,  329,   59,  816, 1353,  743,\n",
       "         597, 1557, 1124,  379,  210,  588, 1450, 1257, 1154,  478,  108,\n",
       "         914,  833,  751, 1078, 1564, 1397,  127, 1431, 1574,  697,  339,\n",
       "        1020,  615,   55,  660, 1010,  364, 1102,  700,   97,  993, 1204,\n",
       "        1509,  590,  503,  622, 1012,  827,  826,  703,  156,  926, 1061,\n",
       "         994, 1148, 1024,  963,  518, 1264,  443,  635,    6, 1228,  381,\n",
       "        1235,  601, 1093, 1333,  594, 1399,  855,  344,  656, 1437, 1019,\n",
       "          67, 1150,  738,  650,  436,  961,  250,  308,  910, 1368, 1473,\n",
       "        1213,  629,  479, 1488,  769,  215, 1563, 1413, 1352, 1552, 1566,\n",
       "         291,  705, 1243, 1072,  293,  226,  541, 1037,  641, 1089,  737,\n",
       "         216]),\n",
       " array([ 719, 1013, 1477,  690,  633,  173,  324, 1255, 1384,  213,  489,\n",
       "         956, 1152, 1425,   86,  321,  323,  269,  995,  205,  753, 1193,\n",
       "        1279,  426,    3, 1128, 1158, 1546,  673,  183,  564,  821, 1418,\n",
       "        1467,  356,  400, 1096, 1222,  897, 1123,   26,   24,  740,  669,\n",
       "        1320,  473,  246,  200,  106,   89, 1585,  180,  462,  349, 1048,\n",
       "        1149,  860,  713, 1028,  440,  302,  167,  407, 1499,  413, 1528,\n",
       "         730, 1374, 1055, 1523, 1452,  454,  372,  990,  806,  589,  940,\n",
       "         222,  680,  238,  600,  316, 1506, 1033, 1345, 1390, 1095,   17,\n",
       "         485, 1342,  908,  616,  971, 1003,  452,  430,  931,  412, 1529,\n",
       "          44,  394,  695, 1192,  289,  970, 1006,  913, 1261,  606, 1011,\n",
       "         813,  262, 1436,  795, 1350, 1205,   99, 1381,  305, 1268,  979,\n",
       "         565, 1568,  351, 1457, 1348,   36,  996,  825,  123,  228, 1396,\n",
       "         365,  284,  487, 1583,  295, 1598,  873,  255,  104, 1225,  612,\n",
       "        1328,  437,  801, 1060,  953,  117,  391, 1254, 1134,  598,  491,\n",
       "         614, 1088,  917,  987,  850,  166,  212,  840,  272, 1497, 1113,\n",
       "         458,  539,  694,  399,  919, 1179,  775, 1214,   61,  779,  688,\n",
       "          78,  893, 1549, 1142, 1361,  138,  420,  439,  408,  578,  777,\n",
       "         292,  459,   34, 1160,  542,  881,  997,  256,  370,   80, 1456,\n",
       "         496,  728, 1021,  701,  445,  792, 1146,  490,  933, 1391,  516,\n",
       "         374, 1468, 1008,  973,  432, 1085,  275, 1046,  546, 1167,  281,\n",
       "          70,  471,  645,  924,  709,  962,  274, 1579, 1301,   38,  335,\n",
       "        1244,  540,  942,  382,  482,  884,  239,  726, 1005,  333, 1363,\n",
       "        1159,  203, 1030,  718, 1283,  330, 1420, 1518,  242,  409,  568,\n",
       "         326,  217,  648,   47, 1304, 1277,  593,  756,  145,  288, 1057,\n",
       "         559, 1212,   58,  796, 1536,  220, 1230,  231,  925,  864,  146,\n",
       "        1459,  447,   83, 1240,  853,  243,  770, 1076, 1382,  264, 1364,\n",
       "         181,  794, 1386, 1032,  575,  885, 1387, 1131,  519, 1471, 1272,\n",
       "        1110,  303, 1105, 1389, 1281,  898,  785,   87,  211, 1414,  666,\n",
       "         980, 1475, 1215,    5, 1454,   19,  570,  464,  311, 1502,  266,\n",
       "        1569]),\n",
       " array([ 136,  116,  460, 1543,  620,  991,  348, 1498,  977, 1481,  486,\n",
       "         711, 1182, 1184,  404,  429, 1504,  722,  517, 1180,  768,  872,\n",
       "          45,  419,  466,  583,  710,  444,  667,  809,  689,  383,  757,\n",
       "         985, 1597,  752, 1419,  306,  481,  161, 1380,  922, 1075,  851,\n",
       "         448, 1043,  651, 1185,  345,  812,  870, 1074, 1099, 1200,  608,\n",
       "         168, 1406, 1325,  911,   40, 1369, 1054,  153,   39,   30,  750,\n",
       "          25,  731,  999, 1476, 1132, 1388,  817,  387,  244, 1066, 1548,\n",
       "         376, 1372, 1337,  248,  174, 1592, 1263,  325,   43,  678,  455,\n",
       "        1570,  162,  715, 1163,  414,  791, 1485,  945,  114, 1079,  493,\n",
       "        1453,  789, 1376,  965,  802,  389,  223, 1305,  890,  169,  748,\n",
       "         497, 1556,  522, 1316, 1576, 1129,  712, 1356, 1315, 1360, 1220,\n",
       "         916,  803, 1067,  524, 1039,  721, 1259,  976,  745,  525,  470,\n",
       "         337,  566,   41, 1081,  422, 1203, 1173, 1023,  704, 1578,  625,\n",
       "         915,   52,  254,  875,  195, 1466,  644,   65, 1520, 1156,  423,\n",
       "         398,  783,  634,  283,  664, 1501, 1252, 1531, 1273, 1062, 1199,\n",
       "        1354, 1151, 1330, 1385,  968,  717,  903, 1465, 1141,  425, 1435,\n",
       "         410,  632,  257, 1250, 1234,  552, 1441,  828,  197, 1040, 1327,\n",
       "        1594,   53, 1143, 1554,  854, 1165, 1026, 1299,  687,  602,  386,\n",
       "         572, 1045, 1335, 1016,  804,  952,  492, 1417,  696,  340, 1109,\n",
       "         935,  165,  435, 1511, 1210,  484, 1238, 1270, 1571, 1402,  154,\n",
       "        1059,  591,  342, 1580,  378, 1027,  300,  124, 1051,   66,  118,\n",
       "           2,  550,  179, 1227,  912,  411,  892,   50, 1393,  837,  249,\n",
       "        1409,  637,  346,  532, 1514,  771,  271,  343,  277,  465, 1329,\n",
       "         573,  279, 1503,  367,   22,  759,   51,  835,   85,  844,  843,\n",
       "         983, 1186,  998, 1015,  509, 1140,  569, 1058,  352,  693,  163,\n",
       "           7,  483,  172, 1508,  513, 1448, 1195, 1535, 1137,  247,  630,\n",
       "         235, 1017, 1144,  653,  949,  421,  727,  538, 1474, 1068,  617,\n",
       "        1218, 1241, 1545, 1338,  708,  405,  170, 1258,  763, 1108, 1246,\n",
       "        1366,  201,  871, 1410,  534,  301, 1542, 1221,  975,  888,  951])]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([1084,  741,  520, ...,  975,  888,  951]),\n",
       " array([1455,  788,  355, ...,  975,  888,  951]),\n",
       " array([1455,  788,  355, ...,  975,  888,  951]),\n",
       " array([1455,  788,  355, ...,  975,  888,  951]),\n",
       " array([1455,  788,  355, ..., 1502,  266, 1569])]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 7.4    0.7    0.    ...  0.56   9.4    5.   ]\n",
      " [ 7.8    0.88   0.    ...  0.68   9.8    5.   ]\n",
      " [ 7.8    0.76   0.04  ...  0.65   9.8    5.   ]\n",
      " ...\n",
      " [ 6.3    0.51   0.13  ...  0.75  11.     6.   ]\n",
      " [ 5.9    0.645  0.12  ...  0.71  10.2    5.   ]\n",
      " [ 6.     0.31   0.47  ...  0.66  11.     6.   ]]\n"
     ]
    }
   ],
   "source": [
    "dataset = data.values\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset[:, :11]\n",
    "y = dataset[:, 11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sgd = linear_model.SGDRegressor(max_iter=10)\n",
    "\n",
    "total_sgd = 0.0\n",
    "X_norm = tf.standardize(X)\n",
    "for i in range(k):\n",
    "    sgd.fit(X_norm[idx_train[i]],y[idx_train[i]])\n",
    "    y_pred = sgd.predict(X_norm[idx_test[i]])\n",
    "    total_sgd = total_sgd + metrics.mean_squared_error(y[idx_test[i]],y_pred)\n",
    "    \n",
    "total_sgd = total_sgd / k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = linear_model.LinearRegression()\n",
    "\n",
    "total_lr = 0.0\n",
    "for i in range(k):\n",
    "    lr.fit(X[idx_train[i]],y[idx_train[i]])\n",
    "    y_pred = lr.predict(X[idx_test[i]])\n",
    "    total_lr = total_lr + metrics.mean_squared_error(y[idx_test[i]],y_pred)\n",
    "\n",
    "total_lr = total_lr / k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsvr = LinearSVR()\n",
    "\n",
    "total_lsvr = 0.0\n",
    "for i in range(k):\n",
    "    lsvr.fit(X[idx_train[i]],y[idx_train[i]])\n",
    "    y_pred = lsvr.predict(X[idx_test[i]])\n",
    "    total_lsvr = total_lsvr + metrics.mean_squared_error(y[idx_test[i]],y_pred)\n",
    "\n",
    "total_lsvr = total_lsvr / k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "svr = SVR()\n",
    "\n",
    "total_svr = 0.0\n",
    "for i in range(k):\n",
    "    svr.fit(X[idx_train[i]],y[idx_train[i]])\n",
    "    y_pred = svr.predict(X[idx_test[i]])\n",
    "    total_svr = total_svr + metrics.mean_squared_error(y[idx_test[i]],y_pred)\n",
    "\n",
    "total_svr = total_svr / k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfr = RandomForestRegressor(max_depth=2, random_state=0)\n",
    "\n",
    "total_rfr = 0.0\n",
    "for i in range(k):\n",
    "    rfr.fit(X[idx_train[i]],y[idx_train[i]])\n",
    "    y_pred = rfr.predict(X[idx_test[i]])\n",
    "    total_rfr = total_rfr + metrics.mean_squared_error(y[idx_test[i]],y_pred)\n",
    "\n",
    "total_rfr = total_rfr / k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbr = GradientBoostingRegressor()\n",
    "\n",
    "total_gbr = 0.0\n",
    "for i in range(k):\n",
    "    gbr.fit(X[idx_train[i]],y[idx_train[i]])\n",
    "    y_pred = gbr.predict(X[idx_test[i]])\n",
    "    total_gbr = total_gbr + metrics.mean_squared_error(y[idx_test[i]],y_pred)\n",
    "    \n",
    "total_gbr = total_gbr / k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 03. Faça um gráfico comparativo entre resultados das avaliações (Evaluation) dos modelos acima."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD: 0.4282349617439386\n",
      "LR: 0.4253300113099633\n",
      "LSVR: 1.0197250815751977\n",
      "SVR: 0.47553976257543634\n",
      "RFR: 0.4629565831512334\n",
      "GBR: 0.392735823920325\n"
     ]
    }
   ],
   "source": [
    "class_names = ['SGD','LR','LSVR','SVR','RFR','GBR']\n",
    "class_ = [total_sgd,total_lr,total_lsvr,total_svr,total_rfr,total_gbr]\n",
    "for i in range(0,len(class_names)):\n",
    "    print(class_names[i] + ': ' + str(class_[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2d40a890a20>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.grid()\n",
    "plt.barh(range(len(class_names)),class_)\n",
    "plt.yticks(range(len(class_names)), class_names);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 04. Escolha o melhor algoritmo obtido a partir de cross validation e treine um modelo usando o dataset completo, ou seja, gere um modelo final."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
       "             learning_rate=0.1, loss='ls', max_depth=3, max_features=None,\n",
       "             max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "             min_impurity_split=None, min_samples_leaf=1,\n",
       "             min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "             n_estimators=100, presort='auto', random_state=None,\n",
       "             subsample=1.0, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_finalGBR = GradientBoostingRegressor()\n",
    "model_finalGBR.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 05. Qual a diferença entre Stochastic Gradient Descent e Gradient Descent? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch gradient descent computes the gradient using the whole dataset. This is great for convex, or relatively smooth error manifolds. In this case, we move somewhat directly towards an optimum solution, either local or global. Additionally, batch gradient descent, given an annealed learning rate, will eventually find the minimum located in it's basin of attraction.\n",
    "\n",
    "Stochastic gradient descent (SGD) computes the gradient using a single sample. Most applications of SGD actually use a minibatch of several samples, for reasons that will be explained a bit later. SGD works well (Not well, I suppose, but better than batch gradient descent) for error manifolds that have lots of local maxima/minima. In this case, the somewhat noisier gradient calculated using the reduced number of samples tends to jerk the model out of local minima into a region that hopefully is more optimal. Single samples are really noisy, while minibatches tend to average a little of the noise out. Thus, the amount of jerk is reduced when using minibatches. A good balance is struck when the minibatch size is small enough to avoid some of the poor local minima, but large enough that it doesn't avoid the global minima or better-performing local minima. (Incidently, this assumes that the best minima have a larger and deeper basin of attraction, and are therefore easier to fall into.)\n",
    "\n",
    "One benefit of SGD is that it's computationally a whole lot faster. Large datasets often can't be held in RAM, which makes vectorization much less efficient. Rather, each sample or batch of samples must be loaded, worked with, the results stored, and so on. Minibatch SGD, on the other hand, is usually intentionally made small enough to be computationally tractable.\n",
    "\n",
    "Usually, this computational advantage is leveraged by performing many more iterations of SGD, making many more steps than conventional batch gradient descent. This usually results in a model that is very close to that which would be found via batch gradient descent, or better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O Gradiente descendente visa a minimização da função.\n",
    "\n",
    "O termo estocástico vem do fato de que o gradiente baseado em uma única amostra de treinamento é uma aproximação estocástica do gradiente de custo verdadeiro. O gradiente descendente estocástico avalia e atualiza os coeficientes a cada iteração para minimizar o erro de um modelo."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
